{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as transform\n",
    "import torch\n",
    "import torch as t\n",
    "import itertools\n",
    "import EnsembleXAI.Metrics as M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading n01491361_9052.JPEG to ./images\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/82.7k [00:00<?, ?B/s]\n",
      "100%|##########| 82.7k/82.7k [00:00<00:00, 4.11MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions download -f \"/ILSVRC/Data/CLS-LOC/train/n01491361/n01491361_9052.JPEG\" -p ./images -c imagenet-object-localization-challenge --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 480, 640])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open('PATH\\\\ImageNetS50\\\\train-semi-segmentation\\\\n01491361\\\\n01491361_9052.png')\n",
    "x = transform.to_tensor(image)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.all(x[0] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logical_mask = t.any(x != 0, dim=-3, keepdim=True).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.all(logical_mask == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logical_mask.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(logical_mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img = Image.open(\"PATH\\\\ImageNet\\\\n01491361\\\\n01491361_13773.JPEG\")\n",
    "x = transform.to_tensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 640])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logical_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reshaped_mask = logical_mask.repeat(3,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x[reshaped_mask] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0][180][250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Proper tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms.functional as transform\n",
    "import torch\n",
    "import torch as t\n",
    "import itertools\n",
    "import EnsembleXAI.Metrics as M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True, False],\n",
       "         [False,  True, False],\n",
       "         [False,  True, False]],\n",
       "\n",
       "        [[False,  True, False],\n",
       "         [False,  True, False],\n",
       "         [False,  True, False]]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = torch.BoolTensor([[[False, True, False]]]).repeat(2,3,1)\n",
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3, 3, 3]), torch.Size([2, 3, 3]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = torch.Tensor([[[[1,2,3], [4,5,6], [7,8,9]], [[1,2,3], [4,5,6], [7,8,9]], [[1,2,3], [4,5,6], [7,8,9]]]]).repeat(2,1,1,1)\n",
    "masks = torch.BoolTensor([[[False, True, False], [False, True, False], [False, True, False]]]).repeat(2,1,1)\n",
    "images.shape, masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = M.replace_masks(images, masks, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(x[:, :, :, 1]==0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(x[:, :, :, [0,2]] == images[:, :, :, [0,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def replace_masks(images: torch.Tensor, masks: torch.Tensor, value = 0) -> torch.Tensor:\n",
    "    temp_images = torch.clone(images)\n",
    "    reshaped_masks = masks.unsqueeze(dim=1).repeat(1, 3, 1, 1)\n",
    "    temp_images[reshaped_masks] = value\n",
    "    return temp_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[False,  True, False],\n",
       "         [False,  True, False],\n",
       "         [False,  True, False]],\n",
       "\n",
       "        [[False,  True, False],\n",
       "         [False,  True, False],\n",
       "         [False,  True, False]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 3, 3])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[torch.Size([3, 3]),\n",
       " torch.Size([3, 3]),\n",
       " torch.Size([3, 3]),\n",
       " torch.Size([3, 3]),\n",
       " torch.Size([3, 3]),\n",
       " torch.Size([3, 3])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.shape for x in M.tensor_to_list_tensors(images, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.linalg.matrix_norm(torch.ones([3,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16.)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.matrix_2_norm(2*torch.ones(4,4,4), torch.zeros(4,4,4), sum_dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plus_2d = torch.Tensor([[0, 1, 0],\n",
    "                                [1, 1, 1],\n",
    "                                [0, 1, 0]])\n",
    "cross_2d = torch.Tensor([[1, 0, 1],\n",
    "                         [0, 1, 0],\n",
    "                         [1, 0, 1]])\n",
    "intersect = M.intersection_mask(plus_2d, cross_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "intersect[1,1] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.any(intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plus_threshold_2d = torch.Tensor([[0, 0.2, 0],\n",
    "                                          [0.5, 0.7, 0.5],\n",
    "                                          [0, 0.2, 0]])\n",
    "blus_threshold_2d = torch.Tensor([[0, 1, 0],\n",
    "                                  [1, 1, 1],\n",
    "                                  [0, 1, 0]])\n",
    "intersect = M.intersection_mask(plus_threshold_2d, blus_threshold_2d, threshold1=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [ True,  True,  True],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([True, True, True])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersect[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plus_2d = torch.Tensor([[0, 1, 0],\n",
    "                        [1, 1, 1],\n",
    "                        [0, 1, 0]])\n",
    "cross_2d = torch.Tensor([[1, 0, 1],\n",
    "                         [0, 1, 0],\n",
    "                         [1, 0, 1]])\n",
    "union = M.union_mask(plus_2d, cross_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "union_threshold_a = torch.Tensor([[1, 0.4, 1],\n",
    "                                  [1, 0.4, 1],\n",
    "                                  [0, 0, 0]])\n",
    "union_threshold_b = torch.Tensor([[0, 0, 0],\n",
    "                                  [0, 0, 0],\n",
    "                                  [0.6, 0.6, 0.6]])\n",
    "union = M.union_mask(union_threshold_a, union_threshold_b, threshold1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [ True, False,  True],\n",
       "        [ True,  True,  True]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_union_05 = torch.BoolTensor([[True, False,  True],\n",
    "                                     [True, False,  True],\n",
    "                                     [True,  True,  True]])\n",
    "torch.all(union==correct_union_05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ones = torch.Tensor([1]).repeat(3, 4, 10, 10)\n",
    "twos = 2*torch.Tensor([1]).repeat(3, 4, 10, 10)\n",
    "tensor = torch.cat([ones, twos])\n",
    "xyz = M.tensor_to_list_tensors(tensor, depth=2)\n",
    "diffs = [\n",
    "    M.matrix_2_norm(exp1, exp2).item()\n",
    "    for exp1, exp2 in itertools.combinations(xyz, 2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 4, 10, 10])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([ones, twos]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090909879069752"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1/(max(diffs)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09090910106897354"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.consistency(tensor)\n",
    "# test confidence impact ratio i decision impact ratio i stability i wszystko xd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# TEST STABILITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function EnsembleXAI.Metrics.stability(explanator: Callable, image: torch.Tensor, images_to_compare: torch.Tensor, epsilon: float = 0.1) -> torch.Tensor>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_dummy(tensor: torch.Tensor, classes=1000):\n",
    "    return torch.ones([tensor.shape[0], classes])/classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "images_tensor = torch.ones([n, 3, 32, 32])\n",
    "expls = torch.randn([n, 1, 32, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a, b = M._impact_ratio_helper(images_tensor, predict_dummy, expls, 0, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.all(a==0.0010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c, d = torch.max(a, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(d == torch.zeros([100])).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "images_tensor = torch.ones([n, 3, 32, 32])\n",
    "expls = torch.randn([n, 1, 32, 32])\n",
    "baseline = 0\n",
    "threshold = 0.5\n",
    "org = M.decision_impact_ratio(images_tensor, predict_dummy, expls, threshold, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 1000])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "images_tensor = torch.ones([n, 3, 32, 32])\n",
    "expls = torch.randn([n, 1, 32, 32])\n",
    "baseline = 0\n",
    "threshold = 0.5\n",
    "value = M.confidence_impact_ratio(images_tensor, predict_dummy, expls, threshold, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_explanation = plus_2d.unsqueeze(0).unsqueeze(0)/2\n",
    "simple_mask = plus_2d.unsqueeze(0)\n",
    "M.accordance_recall(simple_explanation, simple_mask, threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def val(expl, mask, metric):\n",
    "    expl = expl.unsqueeze(0).unsqueeze(0)\n",
    "    mask = mask.unsqueeze(0)\n",
    "    return metric(expl, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val(plus_2d, cross_2d, M.accordance_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val(cross_2d, plus_2d, M.accordance_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val(plus_2d, plus_2d, M.accordance_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000298023224"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val(plus_2d, cross_2d, M.accordance_precision).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000000298023224"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val(cross_2d, plus_2d, M.accordance_precision).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val(plus_2d, plus_2d, M.accordance_precision).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "expl = torch.cat((plus_2d.unsqueeze(0).unsqueeze(0), cross_2d.unsqueeze(0).unsqueeze(0)))\n",
    "mask = torch.cat((cross_2d.unsqueeze(0), plus_2d.unsqueeze(0)))\n",
    "acc_recall = M.accordance_recall(expl, mask)\n",
    "acc_prec = M.accordance_precision(expl, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values = 2 * (acc_recall * acc_prec) / (acc_recall + acc_prec)\n",
    "value = torch.sum(values) / values.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.2000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.20000001788139343"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.F1_score(expl, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1111111119389534"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.intersection_over_union(expl, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_prec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "squeezed_expl = expl.squeeze(dim=1)\n",
    "overlaping_area = M.intersection_mask(squeezed_expl, mask, threshold1=threshold)\n",
    "divisor = torch.sum(torch.abs(squeezed_expl) > threshold, dim=(-2, -1))\n",
    "value = torch.sum(overlaping_area, dim=(-2, -1)) / divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(overlaping_area, dim=(-2, -1))/divisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_dummy2(input_tensor, n_classes = 1000):\n",
    "    n = input_tensor.shape[0]\n",
    "    half_classes = n_classes//2\n",
    "    third_classes = n_classes//3\n",
    "    quarter_classes = n_classes//4\n",
    "    if torch.sum(input_tensor).item() > 300000:\n",
    "        preds = torch.cat([torch.zeros([n, n_classes - quarter_classes]), 0.004*torch.ones([n, quarter_classes])], dim=1)\n",
    "    elif torch.sum(input_tensor).item() > 200000:\n",
    "        preds = torch.cat([torch.zeros([n, n_classes - third_classes]), 0.0033*torch.ones([n, third_classes])], dim=1)\n",
    "    else:\n",
    "        preds = torch.cat([torch.zeros([n, n_classes - half_classes]), 0.002*torch.ones([n, half_classes])], dim=1)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "images_tensor = torch.ones([n, 3, 32, 32])\n",
    "expls = torch.Tensor([0, 0.3, 0.6, 1]).repeat(8).repeat(n, 3, 32, 1)\n",
    "val = M.decision_impact_ratio(images_tensor, predict_dummy2, expls, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0020000000949949026"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val1 = Metrics.confidence_impact_ratio(images_tensor, predict_dummy2, expls, 0, 0.4)\n",
    "val2 = Metrics.confidence_impact_ratio(images_tensor, predict_dummy2, expls, 0, 0.7)\n",
    "val3 = Metrics.confidence_impact_ratio(images_tensor, predict_dummy2, expls, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007000001496635377"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.confidence_impact_ratio(images_tensor, predict_dummy2, expls, 0, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.confidence_impact_ratio(images_tensor, predict_dummy2, expls, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = images_tensor.shape[0]\n",
    "# predictor returns probabilities in a tensor format\n",
    "probs_original, probs_modified = M._impact_ratio_helper(\n",
    "    images_tensor, predict_dummy2, expls, 0.4, 1\n",
    ")\n",
    "_, preds_original = torch.max(probs_original, 1)\n",
    "_, preds_modified = torch.max(probs_modified, 1)\n",
    "value = torch.sum((preds_original != preds_modified).float()) / n\n",
    "value.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum((preds_original != preds_original).float()) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500, 500,\n",
       "        500, 500])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([556, 466, 330,  63, 191, 248, 247, 597, 627, 593, 342,  88, 805,  48,\n",
       "        497, 927, 414,  10, 172, 483, 322, 541, 546, 812, 345, 306, 973, 720,\n",
       "        791, 280, 142, 361, 341,  90, 440, 569, 473, 408, 178, 809,  85, 856,\n",
       "        486, 507, 685, 846,  70, 423, 150, 618, 332, 438, 229, 350, 505, 687,\n",
       "        580, 922, 759, 782,  95, 517,  55, 106, 279, 170, 481, 625, 140, 174,\n",
       "        815, 747, 987,  27, 651, 222, 143, 993, 590, 221, 722, 427, 159, 941,\n",
       "        925,  20, 207, 230, 774, 193, 412, 596, 695, 908, 617,  25, 863, 628,\n",
       "        296, 775, 691, 660, 565, 594, 990, 688, 595, 708,   1, 948, 545,  93,\n",
       "        398, 642, 474,  14, 634, 581, 970, 820, 454,  81, 111, 723, 269, 255,\n",
       "        553,  38, 587, 681, 301,  66, 124, 935, 883, 647, 773, 844, 311, 664,\n",
       "        869, 656, 853,  43, 224,  26, 392, 500, 323, 919,  29, 694,  87, 540,\n",
       "        740, 967, 793, 889, 674, 435, 395, 356, 963, 145, 547, 710, 756, 792,\n",
       "        607, 952, 499, 934, 234, 271, 119, 989, 104, 671, 112, 717, 364, 732,\n",
       "        205, 649, 134, 969, 476, 988, 751, 128, 308, 275, 471, 898, 544, 219,\n",
       "        422, 415, 365, 195, 333, 445, 249, 289, 659, 778, 860, 562, 786, 472,\n",
       "        123, 576, 975, 410, 155, 652, 316,  62, 555, 177, 680, 227, 312, 425,\n",
       "        182, 290, 339, 401, 475, 165,  82,  42, 721, 527, 991, 218, 265,  24,\n",
       "        572, 536, 461, 110,  53, 439, 997, 368, 511, 847, 744, 152, 579, 250,\n",
       "        314, 624, 737, 329, 413, 829, 324, 905, 117, 831, 897, 697,  33, 612,\n",
       "        895, 402, 115, 760, 980, 283, 861,  59, 621, 242, 127, 465, 881, 589,\n",
       "        153, 478, 336,  91,  64, 608, 228, 292, 877, 669, 926, 626, 693, 509,\n",
       "        444, 188, 566, 362,  92,  49,  65, 404, 246, 891, 678, 118,  68, 918,\n",
       "        451, 643, 429, 808, 532, 978,  86, 383, 192, 979, 718, 520, 910, 762,\n",
       "        614, 994,  76,  19, 183, 781,  39, 108, 535, 135, 686, 811, 724, 741,\n",
       "        453, 920, 841, 828, 971, 943,   3, 623, 216, 867, 537, 924, 837, 173,\n",
       "        167, 411, 901, 433, 523, 726, 206, 638, 673, 328, 757, 239, 633, 825,\n",
       "        716, 198, 524, 558, 797, 252, 838, 488, 462, 959, 833, 916, 276,  99,\n",
       "        403, 890, 277, 202, 282, 977, 637, 220, 709, 491, 391, 769, 261, 448,\n",
       "        129, 552, 235, 487, 806, 200, 742, 754, 335, 196, 348, 105, 305, 830,\n",
       "        353, 645, 849,   0, 295,  84,  46, 432, 148, 743, 157, 521, 238, 302,\n",
       "        382, 799, 712,   5, 447, 431, 351, 803, 822, 745, 522, 122, 871, 490,\n",
       "        347, 794, 376, 668, 984, 843, 363, 480, 923, 929, 876, 131, 896, 598,\n",
       "        210, 728, 931, 515,  45, 260, 763, 102, 299,  98, 244, 443, 384, 325,\n",
       "        986, 738, 211, 542,  80,  60, 840, 479, 675,  96,  12, 138, 648, 355,\n",
       "        851,  77, 549, 203, 326, 136, 321, 379, 194, 125, 331, 568, 761, 832,\n",
       "        679, 912,  74, 346, 374, 622, 940, 672, 864, 930, 966, 397, 113, 456,\n",
       "        460,  97, 470, 588, 684, 264, 185, 998, 962,  94, 284, 873,   4, 655,\n",
       "        116, 706, 320, 340, 591, 827, 650, 166, 373, 945, 548, 300, 836,  44,\n",
       "        561, 981, 215, 657, 795, 107,  47, 875,  21, 559, 406, 358, 217, 160,\n",
       "        944, 753, 748, 610, 137, 764, 796, 327, 512,  37, 801, 419, 772, 421,\n",
       "        749, 701, 777, 766, 599, 304,  36, 504, 658, 151, 574, 605, 663, 492,\n",
       "        976, 992, 477,  69, 531, 878, 913, 755, 632, 352, 510, 560, 187, 270,\n",
       "        661, 377, 583, 496, 380,  22, 858, 937, 662, 162, 214, 892, 968, 416,\n",
       "        949, 513, 690, 405,  30, 109, 955,  72, 705, 291, 939, 144, 725, 711,\n",
       "        995, 839, 942, 719, 467, 498, 455, 237, 223, 887, 868, 367, 682, 909,\n",
       "        399, 787, 714, 703, 859, 508,  40, 262, 571, 441, 882, 518, 914,  89,\n",
       "         32, 274, 184,  79, 954, 676,  56,  54, 894, 573,  57, 866,  50, 309,\n",
       "        613, 201, 996, 469, 746,  73, 169, 707, 692, 601, 620, 932, 780, 428,\n",
       "        585, 567, 285, 604, 417, 146, 907, 485, 503, 956,   9, 768, 577, 420,\n",
       "        197, 904, 298, 960,  52, 641,   8, 615, 319, 525, 816, 584, 958, 130,\n",
       "        771, 770, 982, 400, 343, 677, 903, 609,  61, 865, 611, 132,  58, 501,\n",
       "        519, 810, 776, 338, 141, 317, 154, 190, 369, 700, 199, 267, 385, 727,\n",
       "        835, 699, 665, 888, 175, 670, 407, 204, 225, 357, 823, 817, 570, 370,\n",
       "        186, 938,  15, 915, 735, 253, 430, 164, 646, 870, 528, 818, 848, 629,\n",
       "        179, 788,  75,  17, 964, 702, 842, 371, 900, 482, 426, 554, 530, 767,\n",
       "        390, 484, 258, 449,  67, 514, 906, 294, 452, 437, 640, 463, 534, 965,\n",
       "        689, 288, 209, 807, 921, 783, 696, 538,   7, 885, 785, 468, 254, 551,\n",
       "        307, 121, 936, 857, 313, 539, 360, 974, 736, 917, 344,  31, 372, 156,\n",
       "         18, 158, 263, 103, 946, 278, 800, 933, 396, 575, 862,  35, 126, 354,\n",
       "        272, 884, 161, 874, 394, 654, 388, 241, 189, 287, 972, 804, 212, 489,\n",
       "        899, 957, 257,  78, 245, 789, 147, 826, 653, 734, 409, 879,  28,  71,\n",
       "        387, 232, 266, 667, 381, 893, 636, 814, 315, 393, 446, 603, 434, 458,\n",
       "        120, 375, 389, 834, 950, 176,  51, 493, 813, 359, 635, 310, 533, 731,\n",
       "        961, 163, 683, 378, 752, 100, 704, 133, 790, 852, 303, 268, 457,  13,\n",
       "        750, 616, 733, 181, 630, 631,   6, 180, 442, 171, 602, 819, 459, 226,\n",
       "        765, 606, 495, 824, 213, 582, 999, 243, 951, 168, 902, 730, 337, 259,\n",
       "        784, 600, 231, 758,  11, 293,  34, 450, 208, 424,  41, 911, 855, 366,\n",
       "        557, 114, 516, 578, 550, 729,   2, 698, 386, 502, 286, 586, 139, 418,\n",
       "        529, 639, 297, 872, 928, 236, 953, 644, 619, 349, 543, 739, 506,  16,\n",
       "        233, 251, 563, 149, 526, 436, 985, 256, 713, 666, 850, 281, 947, 845,\n",
       "        334, 880,  83, 240, 854, 886, 821, 494, 464, 798, 983, 802, 318, 715,\n",
       "        273,  23, 592, 779, 101, 564])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randperm(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 100\n",
    "images_tensor = torch.ones([n, 3, 32, 32])\n",
    "expls = torch.Tensor([0, 0.3, 0.6, 1]).repeat(8).repeat(n, 3, 32, 1)\n",
    "org, mod = M._impact_ratio_helper(\n",
    "    images_tensor, predict_dummy2, expls, 0.4, 0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "org[:, :750]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0020, 0.0020],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0020, 0.0020],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0020, 0.0020],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0020, 0.0020],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0020, 0.0020],\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.0020, 0.0020, 0.0020]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import EnsembleXAI.Metrics as M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "M.intersection_mask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        [1., 0., 1.]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_2d = torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\n",
    "cross_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0.],\n",
       "        [1., 1., 1.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plus_2d = torch.Tensor([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
    "plus_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Tensor"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.intersection_mask(cross_2d, plus_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True],\n",
       "        [True, True, True],\n",
       "        [True, True, True]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.union_mask(cross_2d, plus_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_2d_small = 0.4*cross_2d\n",
    "plus_2d_small = 0.7*plus_2d\n",
    "M.intersection_mask(cross_2d_small, plus_2d_small, threshold1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False],\n",
       "        [False, False, False],\n",
       "        [False, False, False]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.intersection_mask(cross_2d_small, plus_2d_small, threshold1=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ True, False,  True],\n",
       "        [False,  True, False],\n",
       "        [ True, False,  True]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.union_mask(cross_2d_small, plus_2d_small, threshold1=0.0, threshold2=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 5, 5])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack([torch.ones([3,5,5]), 0.5*torch.ones([3,5,5])]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = torch.stack([torch.ones(3, 5, 5), torch.zeros(3,5,5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ex_explanation = torch.Tensor([1, 0, 0, 0, 0]).repeat(2, 3, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predictor(input_tensor):\n",
    "    n = input_tensor.shape[0]\n",
    "    if input_tensor[0,0,0,0].item() == 1:\n",
    "        val = torch.Tensor([0.8, 0.2, 0]).repeat(n, 1)\n",
    "    else:\n",
    "        val = torch.Tensor([0.2, 0.6, 0.2]).repeat(n, 1)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8000, 0.2000, 0.0000],\n",
       "        [0.8000, 0.2000, 0.0000]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19999998807907104"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.confidence_impact_ratio(data, predictor, ex_explanation, 0.5, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_2d = torch.Tensor([[1, 0, 1], [0, 1, 0], [1, 0, 1]])\n",
    "plus_2d = torch.Tensor([[0, 1, 0], [1, 1, 1], [0, 1, 0]])\n",
    "a = torch.stack([cross_2d.repeat(3,1,1), plus_2d.repeat(3,1,1)])\n",
    "b = torch.stack([plus_2d, cross_2d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.accordance_recall(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2000, 0.2000])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.accordance_precision(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333432674408"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.intersection_over_union(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8660, 0.8660, 0.8660, 0.8660])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = torch.Tensor([0, 0.2, 0.4, 0.6, 0.8]).repeat(4,3,5,1)\n",
    "image = torch.Tensor([0.1, 0.3, 0.5, 0.7, 0.9]).repeat(3,5,1)\n",
    "M.matrix_2_norm(images, image, sum_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(120.), tensor(37.5000))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(images), torch.sum(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def _explain_dummy(imagez_tensor):\n",
    "    summed = torch.sum(imagez_tensor)\n",
    "    n = imagez_tensor.shape[0]\n",
    "    if summed > 100:\n",
    "        explanation = torch.Tensor([0,0,0,1,1]).repeat(n, 3, 5, 1)\n",
    "    else:\n",
    "        explanation = torch.Tensor([0,0,1,1,0]).repeat(n, 3, 5, 1)\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13370312750339508"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "M.stability(_explain_dummy, image, images, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
